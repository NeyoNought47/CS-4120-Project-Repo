{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KTnJaK4WTYx3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['.DS_Store', 'es-AR', 'es-CL', 'es-CO', 'es-CR', 'es-DO', 'es-EC', 'es-HN', 'es-NI', 'es-PA', 'es-PE', 'es-PR', 'es-SV', 'es-UY', 'es-VE', 'std_es']\n",
            "The file exists\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "data_path = r\"C:\\Users\\shang\\Desktop\\clean\"\n",
        "\n",
        "if os.path.exists(data_path):\n",
        "    print(os.listdir(data_path))\n",
        "    print(\"The file exists\")\n",
        "else:\n",
        "    print(f\"{data_path} does not exist\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NZSzQqeNU3wF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "regions: ['es-AR', 'es-CL', 'es-CO', 'es-CR', 'es-DO', 'es-EC', 'es-HN', 'es-NI', 'es-PA', 'es-PE', 'es-PR', 'es-SV', 'es-UY', 'es-VE']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "region_data = {}\n",
        "\n",
        "if os.path.exists(data_path):\n",
        "    sub_folders = sorted(os.listdir(data_path))\n",
        "\n",
        "    for folder_name in sub_folders:\n",
        "        folder_full_path = os.path.join(data_path, folder_name)\n",
        "\n",
        "        if os.path.isdir(folder_full_path) and folder_name.startswith('es-'):\n",
        "            path_en = os.path.join(folder_full_path, 'all.en')\n",
        "            path_es = os.path.join(folder_full_path, 'all.es')\n",
        "\n",
        "            if os.path.exists(path_en) and os.path.exists(path_es):\n",
        "                with open(path_en, 'r', encoding='utf-8') as f:\n",
        "                    lines_en = f.read().strip().split('\\n')\n",
        "                with open(path_es, 'r', encoding='utf-8') as f:\n",
        "                    lines_es = f.read().strip().split('\\n')\n",
        "\n",
        "                current_pairs = []\n",
        "                if len(lines_en) == len(lines_es):\n",
        "                    for en, es in zip(lines_en, lines_es):\n",
        "                        if en.strip() and es.strip():\n",
        "                            current_pairs.append([en.strip(), es.strip()])\n",
        "\n",
        "                    region_data[folder_name] = current_pairs\n",
        "\n",
        "\n",
        "print(\"regions:\", list(region_data.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vKWsJP88YL4R"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2  \n",
        "MAX_LENGTH = 20\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"UNK\"} \n",
        "        self.n_words = 3\n",
        "        \n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r\"([.!?¬ø¬°,])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z√°√©√≠√≥√∫√±√Å√â√ç√ì√ö√ë.!?¬ø¬°,]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "           len(p[1].split(' ')) < MAX_LENGTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kY8AcbSfZ3RS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    indexes = []\n",
        "    for word in sentence.split(' '):\n",
        "        if word in lang.word2index:\n",
        "            indexes.append(lang.word2index[word])\n",
        "        else:\n",
        "            indexes.append(UNK_token) \n",
        "    return indexes\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair, input_lang, output_lang):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m8IZE9UmajhO"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size) \n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "        output = embedded\n",
        "\n",
        "        output, hidden = self.rnn(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AqVuSPfPatU2"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "\n",
        "        self.rnn = nn.RNN(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.rnn(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7IpbdsSyexIm"
      },
      "outputs": [],
      "source": [
        "def validate_epoch(encoder, decoder, validation_pairs, input_lang, output_lang, criterion):\n",
        "\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_correct_tokens = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad(): \n",
        "        for pair in validation_pairs:\n",
        "            input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "            target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "\n",
        "            input_length = input_tensor.size(0)\n",
        "            target_length = target_tensor.size(0)\n",
        "\n",
        "            encoder_hidden = encoder.initHidden()\n",
        "            encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
        "\n",
        "            for ei in range(input_length):\n",
        "                encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "                encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "            decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "            decoder_hidden = encoder_hidden \n",
        "\n",
        "            for di in range(target_length):\n",
        "                decoder_output, decoder_hidden, _ = decoder(\n",
        "                    decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "                loss = criterion(decoder_output, target_tensor[di])\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                topv, topi = decoder_output.topk(1)\n",
        "                if topi.item() == target_tensor[di].item():\n",
        "                    total_correct_tokens += 1\n",
        "                \n",
        "                total_tokens += 1\n",
        "\n",
        "                decoder_input = topi.squeeze().detach()\n",
        "\n",
        "                if decoder_input.item() == EOS_token:\n",
        "                    break\n",
        "\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "    avg_acc = total_correct_tokens / total_tokens if total_tokens > 0 else 0\n",
        "    \n",
        "    return avg_loss, avg_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xAIUuDKPhPGB"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def train_step(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "    correct_tokens = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < 0.5 else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            if topi.item() == target_tensor[di].item():\n",
        "                correct_tokens += 1\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di] \n",
        "\n",
        "    else:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            if topi.item() == target_tensor[di].item():\n",
        "                correct_tokens += 1\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length, correct_tokens / target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "doo7gIzEe6pM"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from torch import optim\n",
        "\n",
        "def train_specific_region_and_return_data(region_name, n_epochs=5, learning_rate=0.001, save_dir=\"models\"):\n",
        "\n",
        "    if region_name not in region_data:\n",
        "        print(f\"Error: region {region_name} not found\")\n",
        "        return None\n",
        "    \n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    raw_pairs = region_data[region_name]\n",
        "\n",
        "    input_lang = Lang(\"eng\")\n",
        "    output_lang = Lang(\"spa\")\n",
        "\n",
        "    clean_pairs = []\n",
        "    for en, es in raw_pairs:\n",
        "        clean_en = normalizeString(en)\n",
        "        clean_es = normalizeString(es)\n",
        "        if len(clean_en.split()) < MAX_LENGTH and len(clean_es.split()) < MAX_LENGTH:\n",
        "            clean_pairs.append([clean_en, clean_es])\n",
        "            input_lang.addSentence(clean_en)\n",
        "            output_lang.addSentence(clean_es)\n",
        "\n",
        "    random.shuffle(clean_pairs)\n",
        "    val_split = int(len(clean_pairs) * 0.8)\n",
        "    \n",
        "    train_pairs = clean_pairs[:val_split]\n",
        "    test_pairs = clean_pairs[val_split:] \n",
        "\n",
        "    hidden_size = 256\n",
        "    encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "    decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    steps_per_epoch = 5000\n",
        "    \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        \n",
        "        with tqdm(total=steps_per_epoch, unit=\"step\", desc=f\"Epoch {epoch}\") as pbar:\n",
        "            for i in range(steps_per_epoch):\n",
        "                pair = random.choice(train_pairs)\n",
        "                input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "                target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "                \n",
        "                loss, acc = train_step(input_tensor, target_tensor, encoder, decoder, \n",
        "                                     encoder_optimizer, decoder_optimizer, criterion)\n",
        "                epoch_loss += loss\n",
        "                epoch_acc += acc\n",
        "                pbar.set_postfix({'loss': f'{epoch_loss/(i+1):.3f}', 'acc': f'{epoch_acc/(i+1):.3f}'})\n",
        "                pbar.update(1)\n",
        "            \n",
        "            val_loss, val_acc = validate_epoch(\n",
        "            encoder, decoder, test_pairs[:200], input_lang, output_lang, criterion\n",
        "        )\n",
        "        \n",
        "        print(f\"Epoch {epoch} | Train Loss: {epoch_loss/steps_per_epoch:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    print(f\"{region_name} training completed!\")\n",
        "\n",
        "    save_path = os.path.join(save_dir, f\"model_{region_name}.pt\")\n",
        "    \n",
        "    checkpoint = {\n",
        "        'encoder_state_dict': encoder.state_dict(),\n",
        "        'decoder_state_dict': decoder.state_dict(),\n",
        "        'input_lang': input_lang, \n",
        "        'output_lang': output_lang,\n",
        "        'hidden_size': hidden_size\n",
        "    }\n",
        "    \n",
        "    torch.save(checkpoint, save_path)\n",
        "    print(f\"Model saved to: {save_path}\")\n",
        "    \n",
        "    return encoder, decoder, input_lang, output_lang, test_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "def generate_translations(encoder, decoder, test_pairs, input_lang, output_lang):\n",
        "    sources = []\n",
        "    references = []\n",
        "    predictions = []\n",
        "    \n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    \n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for pair in tqdm(test_pairs):\n",
        "            src_text = pair[0]\n",
        "            ref_text = pair[1]\n",
        "            \n",
        "            input_tensor = tensorFromSentence(input_lang, src_text)\n",
        "            input_length = input_tensor.size(0)\n",
        "            \n",
        "            encoder_hidden = encoder.initHidden()\n",
        "            encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
        "\n",
        "            for ei in range(input_length):\n",
        "                encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "                encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "            decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            decoded_words = []\n",
        "            \n",
        "            for di in range(MAX_LENGTH):\n",
        "                decoder_output, decoder_hidden, _ = decoder(\n",
        "                    decoder_input, decoder_hidden, encoder_outputs)\n",
        "                topv, topi = decoder_output.topk(1)\n",
        "                \n",
        "                if topi.item() == EOS_token:\n",
        "                    break\n",
        "                else:\n",
        "                    decoded_words.append(output_lang.index2word[topi.item()])\n",
        "                \n",
        "                decoder_input = topi.squeeze().detach()\n",
        "            \n",
        "            pred_text = ' '.join(decoded_words)\n",
        "            \n",
        "            sources.append(src_text)\n",
        "            references.append(ref_text)\n",
        "            predictions.append(pred_text)\n",
        "            \n",
        "    return sources, references, predictions\n",
        "\n",
        "import evaluate\n",
        "\n",
        "def compute_metrics(sources, references, predictions):\n",
        "    results = {}\n",
        "\n",
        "    metric_bleu = evaluate.load(\"sacrebleu\")\n",
        "    metric_chrf = evaluate.load(\"chrf\")\n",
        "    metric_meteor = evaluate.load(\"meteor\")\n",
        "    metric_comet  = evaluate.load(\"comet\") \n",
        "\n",
        "    formatted_refs = [[r] for r in references]\n",
        "    \n",
        "    bleu_res = metric_bleu.compute(predictions=predictions, references=formatted_refs)\n",
        "    results['BLEU'] = bleu_res['score']\n",
        "    print(f\"BLEU: {results['BLEU']:.2f}\")\n",
        "\n",
        "    chrf_res = metric_chrf.compute(predictions=predictions, references=formatted_refs)\n",
        "    results['chrF'] = chrf_res['score']\n",
        "    print(f\"chrF: {results['chrF']:.2f}\")\n",
        "\n",
        "\n",
        "    meteor_res = metric_meteor.compute(predictions=predictions, references=references)\n",
        "    results['METEOR'] = meteor_res['meteor']\n",
        "    print(f\"METEOR: {results['METEOR']:.4f}\")\n",
        "\n",
        "\n",
        "    comet_res = metric_comet.compute(predictions=predictions, references=references, sources=sources)\n",
        "    results['COMET'] = comet_res['mean_score']\n",
        "    print(f\"COMET: {results['COMET']:.4f}\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    indexes = []\n",
        "    for word in sentence.split(' '):\n",
        "        if word in lang.word2index:\n",
        "            indexes.append(lang.word2index[word])\n",
        "        else:\n",
        "\n",
        "            continue \n",
        "    return indexes\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:04<00:00, 40.21step/s, loss=5.396, acc=0.193]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 5.3959 | Val Loss: 6.5028 | Val Acc: 0.1504\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:02<00:00, 40.82step/s, loss=5.272, acc=0.224]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 5.2719 | Val Loss: 6.4890 | Val Acc: 0.1632\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.35step/s, loss=5.325, acc=0.231]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 5.3254 | Val Loss: 6.4435 | Val Acc: 0.1576\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:02<00:00, 40.78step/s, loss=5.296, acc=0.229]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 5.2961 | Val Loss: 6.4781 | Val Acc: 0.1533\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:02<00:00, 40.81step/s, loss=5.280, acc=0.236]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 5.2802 | Val Loss: 6.4715 | Val Acc: 0.1395\n",
            "es-SV training completed!\n",
            "Model saved to: models\\model_es-SV.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 151.32it/s]\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\torchmetrics\\utilities\\imports.py:23: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import DistributionNotFound, get_distribution\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd055f6aebcc43acbb8df9090364910d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\shang\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU: 0.74\n",
            "chrF: 13.07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "METEOR: 0.1474\n",
            "COMET: 0.3887\n",
            "BLEU      : 0.7384\n",
            "chrF      : 13.0673\n",
            "METEOR    : 0.1474\n",
            "COMET     : 0.3887\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:00<00:00, 41.34step/s, loss=5.500, acc=0.186]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 5.4999 | Val Loss: 6.8155 | Val Acc: 0.1146\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:00<00:00, 41.47step/s, loss=5.266, acc=0.214]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 5.2662 | Val Loss: 6.6943 | Val Acc: 0.1129\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:02<00:00, 40.97step/s, loss=5.291, acc=0.217]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 5.2905 | Val Loss: 6.6294 | Val Acc: 0.0998\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:01<00:00, 40.99step/s, loss=5.224, acc=0.232]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 5.2241 | Val Loss: 6.6221 | Val Acc: 0.1347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:04<00:00, 40.22step/s, loss=5.175, acc=0.236]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 5.1750 | Val Loss: 6.5997 | Val Acc: 0.1333\n",
            "es-PE training completed!\n",
            "Model saved to: models\\model_es-PE.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 197.75it/s]\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa521c1f84f242c1ad9db69a2ad63c6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\shang\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU: 0.83\n",
            "chrF: 11.08\n",
            "METEOR: 0.1336\n",
            "COMET: 0.4054\n",
            "BLEU      : 0.8321\n",
            "chrF      : 11.0777\n",
            "METEOR    : 0.1336\n",
            "COMET     : 0.4054\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:00<00:00, 41.45step/s, loss=5.493, acc=0.192]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 5.4929 | Val Loss: 6.7210 | Val Acc: 0.1466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:02<00:00, 40.70step/s, loss=5.299, acc=0.225]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 5.2991 | Val Loss: 6.4725 | Val Acc: 0.1522\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:04<00:00, 40.15step/s, loss=5.317, acc=0.234]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 5.3174 | Val Loss: 6.3829 | Val Acc: 0.1774\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:02<00:00, 40.82step/s, loss=5.284, acc=0.238]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 5.2842 | Val Loss: 6.3368 | Val Acc: 0.1945\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:04<00:00, 40.07step/s, loss=5.312, acc=0.245]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 5.3122 | Val Loss: 6.5405 | Val Acc: 0.1485\n",
            "es-NI training completed!\n",
            "Model saved to: models\\model_es-NI.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 183.80it/s]\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b1d5c3e664c4896b5de4c10aaddafad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\shang\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU: 0.85\n",
            "chrF: 11.88\n",
            "METEOR: 0.1583\n",
            "COMET: 0.3989\n",
            "BLEU      : 0.8461\n",
            "chrF      : 11.8804\n",
            "METEOR    : 0.1583\n",
            "COMET     : 0.3989\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:59<00:00, 41.70step/s, loss=5.389, acc=0.208]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 5.3889 | Val Loss: 6.4211 | Val Acc: 0.1695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:04<00:00, 40.16step/s, loss=5.141, acc=0.246]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 5.1408 | Val Loss: 6.3635 | Val Acc: 0.1657\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.33step/s, loss=5.165, acc=0.259]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 5.1646 | Val Loss: 6.3278 | Val Acc: 0.1677\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:02<00:00, 40.87step/s, loss=5.180, acc=0.260]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 5.1799 | Val Loss: 6.5340 | Val Acc: 0.1800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:04<00:00, 40.12step/s, loss=5.133, acc=0.261]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 5.1333 | Val Loss: 6.4571 | Val Acc: 0.1757\n",
            "es-DO training completed!\n",
            "Model saved to: models\\model_es-DO.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 174.41it/s]\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9b5d596c94b4a5eb3111550bf123a55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\shang\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU: 1.53\n",
            "chrF: 13.38\n",
            "METEOR: 0.1740\n",
            "COMET: 0.4117\n",
            "BLEU      : 1.5268\n",
            "chrF      : 13.3809\n",
            "METEOR    : 0.1740\n",
            "COMET     : 0.4117\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:00<00:00, 41.56step/s, loss=5.354, acc=0.196]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 5.3536 | Val Loss: 6.4915 | Val Acc: 0.1102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:01<00:00, 41.09step/s, loss=5.251, acc=0.221]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 5.2510 | Val Loss: 6.4161 | Val Acc: 0.1404\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.35step/s, loss=5.259, acc=0.232]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 5.2591 | Val Loss: 6.4864 | Val Acc: 0.1519\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.51step/s, loss=5.273, acc=0.237]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 5.2731 | Val Loss: 6.4260 | Val Acc: 0.1576\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:02<00:00, 40.78step/s, loss=5.319, acc=0.237]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 5.3190 | Val Loss: 6.7421 | Val Acc: 0.1363\n",
            "es-EC training completed!\n",
            "Model saved to: models\\model_es-EC.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 193.65it/s]\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b69b5a354074a868b4f12f5134ca20e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\shang\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU: 1.30\n",
            "chrF: 12.21\n",
            "METEOR: 0.1453\n",
            "COMET: 0.3975\n",
            "BLEU      : 1.2993\n",
            "chrF      : 12.2056\n",
            "METEOR    : 0.1453\n",
            "COMET     : 0.3975\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:01<00:00, 41.06step/s, loss=5.428, acc=0.194]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 5.4283 | Val Loss: 6.9084 | Val Acc: 0.1062\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:04<00:00, 40.19step/s, loss=5.356, acc=0.224]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 5.3555 | Val Loss: 6.4003 | Val Acc: 0.1609\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:04<00:00, 40.27step/s, loss=5.360, acc=0.236]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 5.3598 | Val Loss: 6.4260 | Val Acc: 0.1618\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:04<00:00, 40.10step/s, loss=5.349, acc=0.244]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 5.3486 | Val Loss: 6.7223 | Val Acc: 0.1267\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:05<00:00, 39.84step/s, loss=5.255, acc=0.240]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 5.2545 | Val Loss: 6.4289 | Val Acc: 0.1506\n",
            "es-PA training completed!\n",
            "Model saved to: models\\model_es-PA.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 172.82it/s]\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "797ba3f3f08145ebb99579a00707c910",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\shang\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU: 0.79\n",
            "chrF: 11.13\n",
            "METEOR: 0.1361\n",
            "COMET: 0.3927\n",
            "BLEU      : 0.7886\n",
            "chrF      : 11.1316\n",
            "METEOR    : 0.1361\n",
            "COMET     : 0.3927\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:59<00:00, 41.79step/s, loss=5.330, acc=0.204]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 5.3298 | Val Loss: 6.4830 | Val Acc: 0.1757\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.47step/s, loss=5.172, acc=0.241]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 5.1725 | Val Loss: 6.2874 | Val Acc: 0.1989\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.61step/s, loss=5.085, acc=0.260]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 5.0855 | Val Loss: 6.3113 | Val Acc: 0.1972\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:05<00:00, 39.84step/s, loss=5.111, acc=0.269]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 5.1111 | Val Loss: 6.3455 | Val Acc: 0.1926\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:04<00:00, 40.05step/s, loss=5.104, acc=0.266]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 5.1036 | Val Loss: 6.2952 | Val Acc: 0.2155\n",
            "es-PR training completed!\n",
            "Model saved to: models\\model_es-PR.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 189.25it/s]\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ea4a283697e4254859c441c9d609ec9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\shang\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU: 1.61\n",
            "chrF: 12.90\n",
            "METEOR: 0.1924\n",
            "COMET: 0.4224\n",
            "BLEU      : 1.6124\n",
            "chrF      : 12.8998\n",
            "METEOR    : 0.1924\n",
            "COMET     : 0.4224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:59<00:00, 41.69step/s, loss=5.401, acc=0.198]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 5.4011 | Val Loss: 6.4730 | Val Acc: 0.1326\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:05<00:00, 39.93step/s, loss=5.294, acc=0.223]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 5.2936 | Val Loss: 6.3154 | Val Acc: 0.1548\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:04<00:00, 40.06step/s, loss=5.301, acc=0.237]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 5.3011 | Val Loss: 6.3622 | Val Acc: 0.1618\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:04<00:00, 40.16step/s, loss=5.279, acc=0.246]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 5.2787 | Val Loss: 6.3129 | Val Acc: 0.1718\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:05<00:00, 39.71step/s, loss=5.263, acc=0.257]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 5.2626 | Val Loss: 6.5905 | Val Acc: 0.1534\n",
            "es-UY training completed!\n",
            "Model saved to: models\\model_es-UY.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 183.93it/s]\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c27396f152042278fcb0b263504abeb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\shang\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU: 0.98\n",
            "chrF: 11.21\n",
            "METEOR: 0.1484\n",
            "COMET: 0.4102\n",
            "BLEU      : 0.9838\n",
            "chrF      : 11.2116\n",
            "METEOR    : 0.1484\n",
            "COMET     : 0.4102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:59<00:00, 41.70step/s, loss=5.396, acc=0.202]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 5.3964 | Val Loss: 6.5900 | Val Acc: 0.1421\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:01<00:00, 41.05step/s, loss=5.204, acc=0.244]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 5.2043 | Val Loss: 6.2965 | Val Acc: 0.1557\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.37step/s, loss=5.191, acc=0.246]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 5.1908 | Val Loss: 6.3499 | Val Acc: 0.1642\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.44step/s, loss=5.164, acc=0.253]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 5.1636 | Val Loss: 6.7625 | Val Acc: 0.1264\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:01<00:00, 41.11step/s, loss=5.522, acc=0.210]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 5.5224 | Val Loss: 6.7897 | Val Acc: 0.1258\n",
            "es-CO training completed!\n",
            "Model saved to: models\\model_es-CO.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 164.67it/s]\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9186b81ab27a4b3c8a3b13018d2d842e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\shang\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU: 0.69\n",
            "chrF: 11.47\n",
            "METEOR: 0.1367\n",
            "COMET: 0.3944\n",
            "BLEU      : 0.6897\n",
            "chrF      : 11.4665\n",
            "METEOR    : 0.1367\n",
            "COMET     : 0.3944\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:02<00:00, 40.69step/s, loss=5.533, acc=0.187]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 5.5330 | Val Loss: 6.2480 | Val Acc: 0.1456\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.53step/s, loss=5.376, acc=0.214]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 5.3756 | Val Loss: 6.5945 | Val Acc: 0.1338\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:04<00:00, 40.18step/s, loss=5.364, acc=0.221]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 5.3640 | Val Loss: 6.6327 | Val Acc: 0.1383\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.34step/s, loss=5.378, acc=0.223]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 5.3780 | Val Loss: 6.7589 | Val Acc: 0.1416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.53step/s, loss=5.463, acc=0.227]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 5.4628 | Val Loss: 6.9680 | Val Acc: 0.1273\n",
            "es-CR training completed!\n",
            "Model saved to: models\\model_es-CR.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 204.75it/s]\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9740ced33ad5432b9761cae16d36ce25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\shang\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU: 0.78\n",
            "chrF: 10.29\n",
            "METEOR: 0.1191\n",
            "COMET: 0.4138\n",
            "BLEU      : 0.7776\n",
            "chrF      : 10.2939\n",
            "METEOR    : 0.1191\n",
            "COMET     : 0.4138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:00<00:00, 41.35step/s, loss=5.321, acc=0.213]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 5.3207 | Val Loss: 6.4905 | Val Acc: 0.1564\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.56step/s, loss=5.235, acc=0.235]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 5.2351 | Val Loss: 6.3895 | Val Acc: 0.1688\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.45step/s, loss=5.163, acc=0.243]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 5.1630 | Val Loss: 6.5858 | Val Acc: 0.1659\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:04<00:00, 40.16step/s, loss=5.200, acc=0.244]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 5.2002 | Val Loss: 6.5727 | Val Acc: 0.1851\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.33step/s, loss=5.116, acc=0.247]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 5.1158 | Val Loss: 6.5899 | Val Acc: 0.1502\n",
            "es-VE training completed!\n",
            "Model saved to: models\\model_es-VE.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 191.70it/s]\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1957bdd89d7642d8a9940f28ec9986b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\shang\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU: 1.17\n",
            "chrF: 12.89\n",
            "METEOR: 0.1692\n",
            "COMET: 0.4229\n",
            "BLEU      : 1.1685\n",
            "chrF      : 12.8941\n",
            "METEOR    : 0.1692\n",
            "COMET     : 0.4229\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:01<00:00, 41.25step/s, loss=5.423, acc=0.204]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 5.4231 | Val Loss: 6.3093 | Val Acc: 0.1489\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:05<00:00, 39.98step/s, loss=5.259, acc=0.239]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 5.2590 | Val Loss: 6.2628 | Val Acc: 0.1474\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:05<00:00, 39.91step/s, loss=5.183, acc=0.253]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 5.1829 | Val Loss: 6.5682 | Val Acc: 0.1377\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.41step/s, loss=5.164, acc=0.255]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 5.1640 | Val Loss: 6.2729 | Val Acc: 0.1611\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:05<00:00, 39.72step/s, loss=5.233, acc=0.260]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 5.2325 | Val Loss: 6.3198 | Val Acc: 0.1770\n",
            "es-AR training completed!\n",
            "Model saved to: models\\model_es-AR.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 181.92it/s]\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d110a18bf7724f8d806a047c7e7a6f80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\shang\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU: 0.88\n",
            "chrF: 11.99\n",
            "METEOR: 0.1498\n",
            "COMET: 0.3941\n",
            "BLEU      : 0.8787\n",
            "chrF      : 11.9911\n",
            "METEOR    : 0.1498\n",
            "COMET     : 0.3941\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:01<00:00, 41.08step/s, loss=5.410, acc=0.199]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 5.4096 | Val Loss: 6.3542 | Val Acc: 0.1477\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:01<00:00, 41.06step/s, loss=5.267, acc=0.228]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 5.2665 | Val Loss: 6.4004 | Val Acc: 0.1585\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.36step/s, loss=5.209, acc=0.242]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 5.2085 | Val Loss: 6.5029 | Val Acc: 0.1583\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:04<00:00, 40.26step/s, loss=5.250, acc=0.250]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 5.2495 | Val Loss: 6.4997 | Val Acc: 0.1631\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.49step/s, loss=5.244, acc=0.249]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 5.2444 | Val Loss: 6.5491 | Val Acc: 0.1614\n",
            "es-HN training completed!\n",
            "Model saved to: models\\model_es-HN.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 177.27it/s]\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba9aebca287146a58e0e4fc07d1793a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\shang\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU: 1.41\n",
            "chrF: 11.76\n",
            "METEOR: 0.1578\n",
            "COMET: 0.4035\n",
            "BLEU      : 1.4066\n",
            "chrF      : 11.7577\n",
            "METEOR    : 0.1578\n",
            "COMET     : 0.4035\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:01<00:00, 41.04step/s, loss=5.396, acc=0.210]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 5.3959 | Val Loss: 6.6346 | Val Acc: 0.1699\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:02<00:00, 40.71step/s, loss=5.157, acc=0.242]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 5.1571 | Val Loss: 6.6642 | Val Acc: 0.1560\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.43step/s, loss=5.123, acc=0.259]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 5.1228 | Val Loss: 6.4261 | Val Acc: 0.1950\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.52step/s, loss=5.140, acc=0.257]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 5.1401 | Val Loss: 6.5160 | Val Acc: 0.1835\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [02:03<00:00, 40.61step/s, loss=5.164, acc=0.259]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 5.1640 | Val Loss: 6.5741 | Val Acc: 0.1858\n",
            "es-CL training completed!\n",
            "Model saved to: models\\model_es-CL.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 186.57it/s]\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\shang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96a062de99ee4de9895f7c5758317fb1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\shang\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU: 0.87\n",
            "chrF: 12.17\n",
            "METEOR: 0.1642\n",
            "COMET: 0.4052\n",
            "BLEU      : 0.8737\n",
            "chrF      : 12.1708\n",
            "METEOR    : 0.1642\n",
            "COMET     : 0.4052\n",
            "Region     BLEU       COMET     \n",
            "es-SV      0.74       0.3887\n",
            "es-PE      0.83       0.4054\n",
            "es-NI      0.85       0.3989\n",
            "es-DO      1.53       0.4117\n",
            "es-EC      1.30       0.3975\n",
            "es-PA      0.79       0.3927\n",
            "es-PR      1.61       0.4224\n",
            "es-UY      0.98       0.4102\n",
            "es-CO      0.69       0.3944\n",
            "es-CR      0.78       0.4138\n",
            "es-VE      1.17       0.4229\n",
            "es-AR      0.88       0.3941\n",
            "es-HN      1.41       0.4035\n",
            "es-CL      0.87       0.4052\n"
          ]
        }
      ],
      "source": [
        "target_regions = [\n",
        "    'es-SV', 'es-PE', 'es-NI', 'es-DO', 'es-EC', \n",
        "    'es-PA', 'es-PR', 'es-UY', 'es-CO', 'es-CR', \n",
        "    'es-VE', 'es-AR', 'es-HN', 'es-CL'\n",
        "]\n",
        "\n",
        "final_results = {}\n",
        "\n",
        "for region in target_regions:\n",
        "    encoder, decoder, lang_in, lang_out, test_data = train_specific_region_and_return_data(\n",
        "        region, n_epochs=5, learning_rate=0.001\n",
        "    )\n",
        "\n",
        "    eval_subset = test_data[:200]\n",
        "    \n",
        "    srcs, refs, preds = generate_translations(encoder, decoder, eval_subset, lang_in, lang_out)\n",
        "    scores = compute_metrics(srcs, refs, preds)\n",
        "    \n",
        "    for metric, score in scores.items():\n",
        "        val = score if isinstance(score, (int, float)) else 0.0\n",
        "        print(f\"{metric:<10}: {val:.4f}\")\n",
        "    \n",
        "    final_results[region] = scores\n",
        "\n",
        "print(f\"{'Region':<10} {'BLEU':<10} {'COMET':<10}\")\n",
        "for region, scores in final_results.items():\n",
        "    bleu = scores.get('BLEU', 0)\n",
        "    comet = scores.get('COMET', 0)\n",
        "    print(f\"{region:<10} {bleu:.2f}       {comet:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "colab_gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.25"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
