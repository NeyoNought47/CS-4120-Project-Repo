{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3507ff25d3a047a88f927eae265a8a45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\shang\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\shang\\anaconda3\\envs\\colab_gpu\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.translate import AlignedSent, IBMModel1, bleu_score, meteor_score, chrf_score\n",
        "from nltk.tokenize import word_tokenize, TreebankWordDetokenizer\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "# import pickle\n",
        "import dill as pickle\n",
        "import sacrebleu\n",
        "import torch\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    from comet import download_model, load_from_checkpoint\n",
        "    COMET_AVAILABLE = True\n",
        "    comet_model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
        "    comet_model = load_from_checkpoint(comet_model_path)\n",
        "except ImportError:\n",
        "    print(\"COMET library not found. Installing: pip install unbabel-comet\")\n",
        "    COMET_AVAILABLE = False\n",
        "except Exception as e:\n",
        "    print(f\"Error loading COMET: {e}. Running without COMET.\")\n",
        "    COMET_AVAILABLE = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = r\"C:\\Users\\shang\\Desktop\\clean\"\n",
        "\n",
        "def load_data(lang_dir):\n",
        "    \"\"\"Loads and tokenizes data from a language directory.\"\"\"\n",
        "    en_path = os.path.join(data_path, lang_dir, 'all.en')\n",
        "    es_path = os.path.join(data_path, lang_dir, 'all.es')\n",
        "    \n",
        "    if not os.path.exists(en_path) or not os.path.exists(es_path):\n",
        "        return None\n",
        "        \n",
        "    with open(en_path, 'r', encoding='utf-8') as f:\n",
        "        en_lines = f.readlines()\n",
        "    with open(es_path, 'r', encoding='utf-8') as f:\n",
        "        es_lines = f.readlines()\n",
        "        \n",
        "    # Basic tokenization\n",
        "    en_tokens = [word_tokenize(line.strip().lower()) for line in en_lines]\n",
        "    es_tokens = [word_tokenize(line.strip().lower()) for line in es_lines]\n",
        "    \n",
        "    # Filter out empty lines\n",
        "    data = []\n",
        "    for en, es in zip(en_tokens, es_tokens):\n",
        "        if en and es:\n",
        "            data.append((en, es))\n",
        "            \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\ndef save_model(model, model_path):\\n    \"\"\"Saves the trained model to disk using pickle.\\n    \\n    Args:\\n        model: Trained IBMModel1 instance\\n        model_path: Path where to save the model\\n    \"\"\"\\n    # Create directory if it doesn\\'t exist\\n    model_dir = os.path.dirname(model_path)\\n    if model_dir:\\n        os.makedirs(model_dir, exist_ok=True)\\n    \\n    with open(model_path, \\'wb\\') as f:\\n        pickle.dump(model, f)\\n    print(f\"Model saved to {model_path}\")\\n'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def train(train_data, iterations=10):\n",
        "    \"\"\"Trains IBM Model 1 on the given training data.\n",
        "    \n",
        "    Args:\n",
        "        train_data: List of (source_tokens, target_tokens) tuples\n",
        "        iterations: Number of EM iterations (default: 10)\n",
        "    \n",
        "    Returns:\n",
        "        Trained IBMModel1 instance\n",
        "    \"\"\"\n",
        "    aligned_corpus = [AlignedSent(target, source) for source, target in train_data]\n",
        "    ibm1 = IBMModel1(aligned_corpus, iterations)\n",
        "\n",
        "    if isinstance(ibm1.translation_table, defaultdict):\n",
        "        ibm1.translation_table.default_factory = None\n",
        "        for v in ibm1.translation_table.values():\n",
        "            if isinstance(v, defaultdict):\n",
        "                v.default_factory = None\n",
        "    \n",
        "    return ibm1\n",
        "\n",
        "def build_translation_dict(model):\n",
        "    \"\"\"Builds a translation dictionary from the trained model.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained IBMModel1 instance\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary mapping source words to target words\n",
        "    \"\"\"\n",
        "    translation_dict = {}\n",
        "    s_to_t_probs = defaultdict(list)\n",
        "    \n",
        "    # Extract translation probabilities\n",
        "    for t in model.translation_table:\n",
        "        for s in model.translation_table[t]:\n",
        "            prob = model.translation_table[t][s]\n",
        "            if prob > 1e-6:\n",
        "                s_to_t_probs[s].append((t, prob))\n",
        "    \n",
        "    # For each source word, pick the target word with highest probability\n",
        "    for s in s_to_t_probs:\n",
        "        best_t = sorted(s_to_t_probs[s], key=lambda x: x[1], reverse=True)[0][0]\n",
        "        translation_dict[s] = best_t\n",
        "    \n",
        "    return translation_dict\n",
        "\n",
        "\n",
        "def translate(model, source_tokens):\n",
        "    \"\"\"Translates source tokens using the trained model.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained IBMModel1 instance\n",
        "        source_tokens: List of source language tokens\n",
        "    \n",
        "    Returns:\n",
        "        List of target language tokens\n",
        "    \"\"\"\n",
        "    translation_dict = build_translation_dict(model)\n",
        "    translated = []\n",
        "    \n",
        "    for word in source_tokens:\n",
        "        if word in translation_dict:\n",
        "            translated.append(translation_dict[word])\n",
        "        else:\n",
        "            translated.append(word)  # Keep original if no translation found\n",
        "    \n",
        "    return translated\n",
        "\n",
        "'''\n",
        "def evaluate(model, test_data):\n",
        "    \"\"\"Evaluates the model on test data using BLEU score.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained IBMModel1 instance\n",
        "        test_data: List of (source_tokens, target_tokens) tuples\n",
        "    \n",
        "    Returns:\n",
        "        BLEU score (float)\n",
        "    \"\"\"\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "    \n",
        "    for source, target in test_data:\n",
        "        translated = translate(model, source)\n",
        "        references.append([target])\n",
        "        hypotheses.append(translated)\n",
        "    \n",
        "    score = bleu_score.corpus_bleu(references, hypotheses)\n",
        "    return score\n",
        "'''\n",
        "\n",
        "def evaluate_metrics(model, test_data):\n",
        "    \"\"\"\n",
        "    Evaluates the model using BLEU, ChrF, METEOR, and COMET.\n",
        "    \"\"\"\n",
        "\n",
        "    detokenizer = TreebankWordDetokenizer()\n",
        "    \n",
        "    refs_tokens_list = [] \n",
        "    hyps_tokens_list = []    \n",
        "    \n",
        "    meteor_scores = []\n",
        "\n",
        "    src_strs = []\n",
        "    ref_strs = []\n",
        "    hyp_strs = []\n",
        "    \n",
        "    for source_tokens, target_tokens in test_data:\n",
        "        hyp_tokens = translate(model, source_tokens)\n",
        "        \n",
        "        refs_tokens_list.append([target_tokens])\n",
        "        hyps_tokens_list.append(hyp_tokens)\n",
        "        \n",
        "        m_score = meteor_score.meteor_score([target_tokens], hyp_tokens)\n",
        "        meteor_scores.append(m_score)\n",
        "        \n",
        "        src_str = detokenizer.detokenize(source_tokens)\n",
        "        ref_str = detokenizer.detokenize(target_tokens)\n",
        "        hyp_str = detokenizer.detokenize(hyp_tokens)\n",
        "        \n",
        "        src_strs.append(src_str)\n",
        "        ref_strs.append(ref_str)\n",
        "        hyp_strs.append(hyp_str)\n",
        "        \n",
        "    results = {}\n",
        "    \n",
        "    bleu = bleu_score.corpus_bleu(refs_tokens_list, hyps_tokens_list)\n",
        "    results['BLEU'] = bleu\n",
        "    \n",
        "    results['METEOR'] = np.mean(meteor_scores)\n",
        "    \n",
        "    chrf = chrf_score.corpus_chrf(ref_strs, hyp_strs)\n",
        "    results['ChrF'] = chrf\n",
        "    \n",
        "    if COMET_AVAILABLE:\n",
        "        print(\"Calculating COMET...\")\n",
        "        data = [{\"src\": s, \"mt\": h, \"ref\": r} for s, h, r in zip(src_strs, hyp_strs, ref_strs)]\n",
        "        try:\n",
        "            comet_output = comet_model.predict(data, batch_size=128, gpus=1)\n",
        "            results['COMET'] = comet_output.system_score\n",
        "        except Exception as e:\n",
        "            print(f\"Error running COMET: {e}\")\n",
        "            results['COMET'] = None\n",
        "    else:\n",
        "        results['COMET'] = None\n",
        "        \n",
        "    return results\n",
        "'''\n",
        "def save_model(model, model_path):\n",
        "    \"\"\"Saves the trained model to disk using pickle.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained IBMModel1 instance\n",
        "        model_path: Path where to save the model\n",
        "    \"\"\"\n",
        "    # Create directory if it doesn't exist\n",
        "    model_dir = os.path.dirname(model_path)\n",
        "    if model_dir:\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "    \n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n# Run for all datasets\\nsubdirs = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\\nresults = {}\\n\\nfor d in sorted(subdirs):\\n    score = train_and_evaluate(d, True)\\n    # if score is not None:\\n        # results[d] = score\\n        \\nprint(\"\\nFinal Results:\")\\n# for lang, score in results.items():\\n    # print(f\"{lang}: {score:.4f}\")\\n'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "def train_and_evaluate(lang_dir, save_model_flag=False):\n",
        "    \"\"\"Convenience function to train and evaluate a model for a language directory.\n",
        "    \n",
        "    Args:\n",
        "        lang_dir: Language directory name\n",
        "        save_model_flag: Whether to save the trained model (default: False)\n",
        "    \n",
        "    Returns:\n",
        "        BLEU score (float) or None if data not found\n",
        "    \"\"\"\n",
        "    print(f\"Processing {lang_dir}...\")\n",
        "    data = load_data(lang_dir)\n",
        "    if not data:\n",
        "        print(f\"Skipping {lang_dir} (files not found or empty)\")\n",
        "        return None\n",
        "        \n",
        "    # Split data\n",
        "    random.seed(42)\n",
        "    random.shuffle(data)\n",
        "    split_idx = int(len(data) * 0.8)\n",
        "    train_data = data[:split_idx]\n",
        "    test_data = data[split_idx:]\n",
        "    \n",
        "    print(f\"  Training on {len(train_data)} sentences...\")\n",
        "    model = train(train_data, iterations=10)\n",
        "    \n",
        "    # Save model if requested\n",
        "\n",
        "    if save_model_flag:\n",
        "        model_path = os.path.join('models', f'{lang_dir}_ibm1.pkl')\n",
        "        save_model(model, model_path)\n",
        "\n",
        "    # print(\"  Evaluating...\")\n",
        "    # score = evaluate(model, test_data)\n",
        "    # print(f\"  BLEU Score: {score:.4f}\")\n",
        "    return # score\n",
        "'''\n",
        "def train_and_evaluate(lang_dir, save_model_flag=False):\n",
        "    \"\"\"Convenience function to train and evaluate a model for a language directory.\n",
        "    \n",
        "    Args:\n",
        "        lang_dir: Language directory name\n",
        "        save_model_flag: Whether to save the trained model (default: False)\n",
        "    \n",
        "    Returns:\n",
        "        BLEU score (float) or None if data not found\n",
        "    \"\"\"\n",
        "    print(f\"Processing {lang_dir}...\")\n",
        "    data = load_data(lang_dir)\n",
        "    if not data:\n",
        "        print(f\"Skipping {lang_dir}\")\n",
        "        return None\n",
        "        \n",
        "    random.seed(42)\n",
        "    random.shuffle(data)\n",
        " \n",
        "\n",
        "    split_idx = int(len(data) * 0.8)\n",
        "    train_data = data[:split_idx]\n",
        "    test_data = data[split_idx:]\n",
        "    \n",
        "    print(f\"Training on {len(train_data)} sentences...\")\n",
        "    model = train(train_data, iterations=10)\n",
        "    '''\n",
        "    if save_model_flag:\n",
        "        model_path = os.path.join('models', f'{lang_dir}_ibm1.pkl')\n",
        "        save_model(model, model_path)\n",
        "    '''\n",
        "    print(\"Evaluating...\")\n",
        "    scores = evaluate_metrics(model, test_data)\n",
        "    \n",
        "    print(\"Results:\")\n",
        "    for k, v in scores.items():\n",
        "        if v is not None:\n",
        "            print(f\"{k}: {v:.4f}\")\n",
        "            \n",
        "    return scores\n",
        "'''\n",
        "# Run for all datasets\n",
        "subdirs = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
        "results = {}\n",
        "\n",
        "for d in sorted(subdirs):\n",
        "    score = train_and_evaluate(d, True)\n",
        "    # if score is not None:\n",
        "        # results[d] = score\n",
        "        \n",
        "print(\"\\nFinal Results:\")\n",
        "# for lang, score in results.items():\n",
        "    # print(f\"{lang}: {score:.4f}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 14 models to evaluate.\n",
            "Processing: es-AR\n",
            "  Loading data...\n",
            "  Final Test set size: 1000\n",
            "  Loading model: es-AR_ibm1.pkl...\n",
            " Evaluating metrics (BLEU, METEOR, ChrF, COMET)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating COMET...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Result for es-AR:\n",
            "    BLEU: 0.1379\n",
            "    METEOR: 0.4618\n",
            "    ChrF: 0.4516\n",
            "    COMET: 0.6551\n",
            "Processing: es-CL\n",
            "  Loading data...\n",
            "  Final Test set size: 1000\n",
            "  Loading model: es-CL_ibm1.pkl...\n",
            " Evaluating metrics (BLEU, METEOR, ChrF, COMET)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating COMET...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Result for es-CL:\n",
            "    BLEU: 0.1379\n",
            "    METEOR: 0.4618\n",
            "    ChrF: 0.4516\n",
            "    COMET: 0.6551\n",
            "Processing: es-CO\n",
            "  Loading data...\n",
            "  Final Test set size: 1000\n",
            "  Loading model: es-CO_ibm1.pkl...\n",
            " Evaluating metrics (BLEU, METEOR, ChrF, COMET)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating COMET...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Result for es-CO:\n",
            "    BLEU: 0.1368\n",
            "    METEOR: 0.4682\n",
            "    ChrF: 0.4565\n",
            "    COMET: 0.6562\n",
            "Processing: es-CR\n",
            "  Loading data...\n",
            "  Final Test set size: 1000\n",
            "  Loading model: es-CR_ibm1.pkl...\n",
            " Evaluating metrics (BLEU, METEOR, ChrF, COMET)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating COMET...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Result for es-CR:\n",
            "    BLEU: 0.1379\n",
            "    METEOR: 0.4618\n",
            "    ChrF: 0.4516\n",
            "    COMET: 0.6551\n",
            "Processing: es-DO\n",
            "  Loading data...\n",
            "  Final Test set size: 1000\n",
            "  Loading model: es-DO_ibm1.pkl...\n",
            " Evaluating metrics (BLEU, METEOR, ChrF, COMET)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating COMET...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Result for es-DO:\n",
            "    BLEU: 0.1379\n",
            "    METEOR: 0.4618\n",
            "    ChrF: 0.4516\n",
            "    COMET: 0.6549\n",
            "Processing: es-EC\n",
            "  Loading data...\n",
            "  Final Test set size: 1000\n",
            "  Loading model: es-EC_ibm1.pkl...\n",
            " Evaluating metrics (BLEU, METEOR, ChrF, COMET)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating COMET...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Result for es-EC:\n",
            "    BLEU: 0.1368\n",
            "    METEOR: 0.4682\n",
            "    ChrF: 0.4565\n",
            "    COMET: 0.6562\n",
            "Processing: es-HN\n",
            "  Loading data...\n",
            "  Final Test set size: 1000\n",
            "  Loading model: es-HN_ibm1.pkl...\n",
            " Evaluating metrics (BLEU, METEOR, ChrF, COMET)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating COMET...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Result for es-HN:\n",
            "    BLEU: 0.1379\n",
            "    METEOR: 0.4618\n",
            "    ChrF: 0.4516\n",
            "    COMET: 0.6551\n",
            "Processing: es-NI\n",
            "  Loading data...\n",
            "  Final Test set size: 1000\n",
            "  Loading model: es-NI_ibm1.pkl...\n",
            " Evaluating metrics (BLEU, METEOR, ChrF, COMET)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating COMET...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Result for es-NI:\n",
            "    BLEU: 0.1379\n",
            "    METEOR: 0.4618\n",
            "    ChrF: 0.4516\n",
            "    COMET: 0.6551\n",
            "Processing: es-PA\n",
            "  Loading data...\n",
            "  Final Test set size: 1000\n",
            "  Loading model: es-PA_ibm1.pkl...\n",
            " Evaluating metrics (BLEU, METEOR, ChrF, COMET)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating COMET...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Result for es-PA:\n",
            "    BLEU: 0.1379\n",
            "    METEOR: 0.4618\n",
            "    ChrF: 0.4516\n",
            "    COMET: 0.6551\n",
            "Processing: es-PE\n",
            "  Loading data...\n",
            "  Final Test set size: 1000\n",
            "  Loading model: es-PE_ibm1.pkl...\n",
            " Evaluating metrics (BLEU, METEOR, ChrF, COMET)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating COMET...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Result for es-PE:\n",
            "    BLEU: 0.1368\n",
            "    METEOR: 0.4682\n",
            "    ChrF: 0.4565\n",
            "    COMET: 0.6562\n",
            "Processing: es-PR\n",
            "  Loading data...\n",
            "  Final Test set size: 1000\n",
            "  Loading model: es-PR_ibm1.pkl...\n",
            " Evaluating metrics (BLEU, METEOR, ChrF, COMET)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating COMET...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Result for es-PR:\n",
            "    BLEU: 0.1368\n",
            "    METEOR: 0.4682\n",
            "    ChrF: 0.4565\n",
            "    COMET: 0.6562\n",
            "Processing: es-SV\n",
            "  Loading data...\n",
            "  Final Test set size: 1000\n",
            "  Loading model: es-SV_ibm1.pkl...\n",
            " Evaluating metrics (BLEU, METEOR, ChrF, COMET)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating COMET...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Result for es-SV:\n",
            "    BLEU: 0.1368\n",
            "    METEOR: 0.4682\n",
            "    ChrF: 0.4565\n",
            "    COMET: 0.6562\n",
            "Processing: es-UY\n",
            "  Loading data...\n",
            "  Final Test set size: 1000\n",
            "  Loading model: es-UY_ibm1.pkl...\n",
            " Evaluating metrics (BLEU, METEOR, ChrF, COMET)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating COMET...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Result for es-UY:\n",
            "    BLEU: 0.1380\n",
            "    METEOR: 0.4621\n",
            "    ChrF: 0.4519\n",
            "    COMET: 0.6550\n",
            "Processing: es-VE\n",
            "  Loading data...\n",
            "  Final Test set size: 1000\n",
            "  Loading model: es-VE_ibm1.pkl...\n",
            " Evaluating metrics (BLEU, METEOR, ChrF, COMET)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating COMET...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Result for es-VE:\n",
            "    BLEU: 0.1368\n",
            "    METEOR: 0.4682\n",
            "    ChrF: 0.4565\n",
            "    COMET: 0.6562\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "# import pickle\n",
        "import dill as pickle\n",
        "\n",
        "model_dir = \"models\"\n",
        "\n",
        "def run_evaluation_pipeline():\n",
        "    model_files = [f for f in os.listdir(model_dir) if f.endswith('_ibm1.pkl')]\n",
        "    \n",
        "    print(f\"Found {len(model_files)} models to evaluate.\")\n",
        "    \n",
        "    final_report = {}\n",
        "\n",
        "    for filename in sorted(model_files):\n",
        "        lang_code = filename.replace('_ibm1.pkl', '')\n",
        "        print(f\"Processing: {lang_code}\")\n",
        "        \n",
        "        print(\"  Loading data...\")\n",
        "        data = load_data(lang_code)\n",
        "        if not data:\n",
        "            continue\n",
        "\n",
        "        random.seed(42)\n",
        "        random.shuffle(data)\n",
        "        split_idx = int(len(data) * 0.8)\n",
        "        test_data = data[split_idx:]\n",
        "        test_data = test_data[:1000]\n",
        "\n",
        "        \n",
        "        print(f\"Final Test set size: {len(test_data)}\")\n",
        "\n",
        "        model_path = os.path.join(model_dir, filename)\n",
        "        print(f\"Loading model: {filename}...\")\n",
        "        try:\n",
        "            with open(model_path, 'rb') as f:\n",
        "                model = pickle.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"  [Error] Failed to load model: {e}\")\n",
        "            continue\n",
        "\n",
        "        print(\" Evaluating metrics (BLEU, METEOR, ChrF, COMET)...\")\n",
        "        scores = evaluate_metrics(model, test_data)\n",
        "        \n",
        "        final_report[lang_code] = scores\n",
        "        print(f\"  Result for {lang_code}:\")\n",
        "        for k, v in scores.items():\n",
        "            if v is not None:\n",
        "                val = v if k == 'COMET' else v \n",
        "                print(f\"    {k}: {val:.4f}\")\n",
        "\n",
        "    return final_report\n",
        "\n",
        "all_results = run_evaluation_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "colab_gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.25"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
