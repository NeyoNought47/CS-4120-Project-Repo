{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.translate import AlignedSent, IBMModel1, bleu_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = 'data/clean'\n",
        "\n",
        "def load_data(lang_dir):\n",
        "    \"\"\"Loads and tokenizes data from a language directory.\"\"\"\n",
        "    en_path = os.path.join(data_path, lang_dir, 'all.en')\n",
        "    es_path = os.path.join(data_path, lang_dir, 'all.es')\n",
        "    \n",
        "    if not os.path.exists(en_path) or not os.path.exists(es_path):\n",
        "        return None\n",
        "        \n",
        "    with open(en_path, 'r', encoding='utf-8') as f:\n",
        "        en_lines = f.readlines()\n",
        "    with open(es_path, 'r', encoding='utf-8') as f:\n",
        "        es_lines = f.readlines()\n",
        "        \n",
        "    # Basic tokenization\n",
        "    en_tokens = [word_tokenize(line.strip().lower()) for line in en_lines]\n",
        "    es_tokens = [word_tokenize(line.strip().lower()) for line in es_lines]\n",
        "    \n",
        "    # Filter out empty lines\n",
        "    data = []\n",
        "    for en, es in zip(en_tokens, es_tokens):\n",
        "        if en and es:\n",
        "            data.append((en, es))\n",
        "            \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate(lang_dir):\n",
        "    print(f\"Processing {lang_dir}...\")\n",
        "    data = load_data(lang_dir)\n",
        "    if not data:\n",
        "        print(f\"Skipping {lang_dir} (files not found or empty)\")\n",
        "        return None\n",
        "        \n",
        "    # Split data\n",
        "    random.seed(42)\n",
        "    random.shuffle(data)\n",
        "    split_idx = int(len(data) * 0.8)\n",
        "    train_data = data[:split_idx]\n",
        "    test_data = data[split_idx:]\n",
        "    \n",
        "    print(f\"  Training on {len(train_data)} sentences...\")\n",
        "    aligned_corpus = [AlignedSent(es, en) for en, es in train_data]\n",
        "    \n",
        "    # Train IBM Model 1\n",
        "    ibm1 = IBMModel1(aligned_corpus, 10)\n",
        "\n",
        "    # Clean model for saving (remove lambdas)\n",
        "    if isinstance(ibm1.translation_table, defaultdict):\n",
        "        ibm1.translation_table.default_factory = None\n",
        "        for v in ibm1.translation_table.values():\n",
        "            if isinstance(v, defaultdict):\n",
        "                v.default_factory = None\n",
        "\n",
        "    # Save model\n",
        "    model_path = os.path.join('saved_models', f'{lang_dir}_ibm1.pt')\n",
        "    torch.save(ibm1, model_path)\n",
        "    print(f\"  Model saved to {model_path}\")\n",
        "\n",
        "    translation_dict = {}\n",
        "\n",
        "    src_vocab = set()\n",
        "    for en, _ in train_data:\n",
        "        src_vocab.update(en)\n",
        "    \n",
        "    s_to_t_probs = defaultdict(list)\n",
        "    \n",
        "    for t in ibm1.translation_table:\n",
        "        for s in ibm1.translation_table[t]:\n",
        "            prob = ibm1.translation_table[t][s]\n",
        "            if prob > 1e-6:\n",
        "                s_to_t_probs[s].append((t, prob))\n",
        "                \n",
        "    for s in s_to_t_probs:\n",
        "        # Sort by prob descending\n",
        "        best_t = sorted(s_to_t_probs[s], key=lambda x: x[1], reverse=True)[0][0]\n",
        "        translation_dict[s] = best_t\n",
        "    \n",
        "    print(\"  Evaluating...\")\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "    \n",
        "    for en, es in test_data:\n",
        "        trans = []\n",
        "        for word in en:\n",
        "            if word in translation_dict:\n",
        "                trans.append(translation_dict[word])\n",
        "            else:\n",
        "                trans.append(word)\n",
        "        \n",
        "        references.append([es])\n",
        "        hypotheses.append(trans)\n",
        "        \n",
        "    score = bleu_score.corpus_bleu(references, hypotheses)\n",
        "    print(f\"  BLEU Score: {score:.4f}\")\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing es-AR...\n",
            "  Training on 115077 sentences...\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "Can't get local object 'IBMModel.reset_probabilities.<locals>.<lambda>'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m results = {}\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(subdirs):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     score = \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      8\u001b[39m         results[d] = score\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(lang_dir)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[32m     29\u001b[39m model_path = os.path.join(\u001b[33m'\u001b[39m\u001b[33msaved_models\u001b[39m\u001b[33m'\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_ibm1.pt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mibm1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Model saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m translation_dict = {}\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cs4120/lib/python3.12/site-packages/torch/serialization.py:967\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cs4120/lib/python3.12/site-packages/torch/serialization.py:1213\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[39m\n\u001b[32m   1210\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m persistent_id(obj)\n\u001b[32m   1212\u001b[39m pickler = PyTorchPickler(data_buf, protocol=pickle_protocol)\n\u001b[32m-> \u001b[39m\u001b[32m1213\u001b[39m \u001b[43mpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1214\u001b[39m data_value = data_buf.getvalue()\n\u001b[32m   1215\u001b[39m zip_file.write_record(\u001b[33m\"\u001b[39m\u001b[33mdata.pkl\u001b[39m\u001b[33m\"\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
            "\u001b[31mAttributeError\u001b[39m: Can't get local object 'IBMModel.reset_probabilities.<locals>.<lambda>'"
          ]
        }
      ],
      "source": [
        "# Run for all datasets\n",
        "subdirs = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
        "results = {}\n",
        "\n",
        "for d in sorted(subdirs):\n",
        "    score = train_and_evaluate(d)\n",
        "    if score is not None:\n",
        "        results[d] = score\n",
        "        \n",
        "print(\"\\nFinal Results:\")\n",
        "for lang, score in results.items():\n",
        "    print(f\"{lang}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'es-AR': 0.1366027570679373,\n",
              " 'es-CL': 0.13660496465381294,\n",
              " 'es-CO': 0.13511849107394103,\n",
              " 'es-CR': 0.13659905452238438,\n",
              " 'es-DO': 0.13661045964864169,\n",
              " 'es-EC': 0.13511849107394103,\n",
              " 'es-HN': 0.13660496465381294,\n",
              " 'es-NI': 0.13660496465381294,\n",
              " 'es-PA': 0.13660496465381294,\n",
              " 'es-PE': 0.13511849107394103,\n",
              " 'es-PR': 0.13511849107394103,\n",
              " 'es-SV': 0.13511849107394103,\n",
              " 'es-UY': 0.13659001270328908,\n",
              " 'es-VE': 0.13511849107394103}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cs4120",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
