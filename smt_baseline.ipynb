{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.translate import AlignedSent, IBMModel1, bleu_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "# import pickle\n",
        "import dill as pickle\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = 'data/clean'\n",
        "\n",
        "def load_data(lang_dir):\n",
        "    \"\"\"Loads and tokenizes data from a language directory.\"\"\"\n",
        "    en_path = os.path.join(data_path, lang_dir, 'all.en')\n",
        "    es_path = os.path.join(data_path, lang_dir, 'all.es')\n",
        "    \n",
        "    if not os.path.exists(en_path) or not os.path.exists(es_path):\n",
        "        return None\n",
        "        \n",
        "    with open(en_path, 'r', encoding='utf-8') as f:\n",
        "        en_lines = f.readlines()\n",
        "    with open(es_path, 'r', encoding='utf-8') as f:\n",
        "        es_lines = f.readlines()\n",
        "        \n",
        "    # Basic tokenization\n",
        "    en_tokens = [word_tokenize(line.strip().lower()) for line in en_lines]\n",
        "    es_tokens = [word_tokenize(line.strip().lower()) for line in es_lines]\n",
        "    \n",
        "    # Filter out empty lines\n",
        "    data = []\n",
        "    for en, es in zip(en_tokens, es_tokens):\n",
        "        if en and es:\n",
        "            data.append((en, es))\n",
        "            \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(train_data, iterations=10):\n",
        "    \"\"\"Trains IBM Model 1 on the given training data.\n",
        "    \n",
        "    Args:\n",
        "        train_data: List of (source_tokens, target_tokens) tuples\n",
        "        iterations: Number of EM iterations (default: 10)\n",
        "    \n",
        "    Returns:\n",
        "        Trained IBMModel1 instance\n",
        "    \"\"\"\n",
        "    aligned_corpus = [AlignedSent(target, source) for source, target in train_data]\n",
        "    ibm1 = IBMModel1(aligned_corpus, iterations)\n",
        "\n",
        "    if isinstance(ibm1.translation_table, defaultdict):\n",
        "        ibm1.translation_table.default_factory = None\n",
        "        for v in ibm1.translation_table.values():\n",
        "            if isinstance(v, defaultdict):\n",
        "                v.default_factory = None\n",
        "    \n",
        "    return ibm1\n",
        "\n",
        "def build_translation_dict(model):\n",
        "    \"\"\"Builds a translation dictionary from the trained model.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained IBMModel1 instance\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary mapping source words to target words\n",
        "    \"\"\"\n",
        "    translation_dict = {}\n",
        "    s_to_t_probs = defaultdict(list)\n",
        "    \n",
        "    # Extract translation probabilities\n",
        "    for t in model.translation_table:\n",
        "        for s in model.translation_table[t]:\n",
        "            prob = model.translation_table[t][s]\n",
        "            if prob > 1e-6:\n",
        "                s_to_t_probs[s].append((t, prob))\n",
        "    \n",
        "    # For each source word, pick the target word with highest probability\n",
        "    for s in s_to_t_probs:\n",
        "        best_t = sorted(s_to_t_probs[s], key=lambda x: x[1], reverse=True)[0][0]\n",
        "        translation_dict[s] = best_t\n",
        "    \n",
        "    return translation_dict\n",
        "\n",
        "\n",
        "def translate(model, source_tokens):\n",
        "    \"\"\"Translates source tokens using the trained model.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained IBMModel1 instance\n",
        "        source_tokens: List of source language tokens\n",
        "    \n",
        "    Returns:\n",
        "        List of target language tokens\n",
        "    \"\"\"\n",
        "    translation_dict = build_translation_dict(model)\n",
        "    translated = []\n",
        "    \n",
        "    for word in source_tokens:\n",
        "        if word in translation_dict:\n",
        "            translated.append(translation_dict[word])\n",
        "        else:\n",
        "            translated.append(word)  # Keep original if no translation found\n",
        "    \n",
        "    return translated\n",
        "\n",
        "\n",
        "def evaluate(model, test_data):\n",
        "    \"\"\"Evaluates the model on test data using BLEU score.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained IBMModel1 instance\n",
        "        test_data: List of (source_tokens, target_tokens) tuples\n",
        "    \n",
        "    Returns:\n",
        "        BLEU score (float)\n",
        "    \"\"\"\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "    \n",
        "    for source, target in test_data:\n",
        "        translated = translate(model, source)\n",
        "        references.append([target])\n",
        "        hypotheses.append(translated)\n",
        "    \n",
        "    score = bleu_score.corpus_bleu(references, hypotheses)\n",
        "    return score\n",
        "\n",
        "\n",
        "def save_model(model, model_path):\n",
        "    \"\"\"Saves the trained model to disk using pickle.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained IBMModel1 instance\n",
        "        model_path: Path where to save the model\n",
        "    \"\"\"\n",
        "    # Create directory if it doesn't exist\n",
        "    model_dir = os.path.dirname(model_path)\n",
        "    if model_dir:\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "    \n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    print(f\"Model saved to {model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing es-AR...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Training on 115077 sentences...\n",
            "Model saved to models/es-AR_ibm1.pkl\n",
            "Processing es-CL...\n",
            "  Training on 115077 sentences...\n",
            "Model saved to models/es-CL_ibm1.pkl\n",
            "Processing es-CO...\n",
            "  Training on 115076 sentences...\n",
            "Model saved to models/es-CO_ibm1.pkl\n",
            "Processing es-CR...\n",
            "  Training on 115077 sentences...\n",
            "Model saved to models/es-CR_ibm1.pkl\n",
            "Processing es-DO...\n",
            "  Training on 115077 sentences...\n",
            "Model saved to models/es-DO_ibm1.pkl\n",
            "Processing es-EC...\n",
            "  Training on 115076 sentences...\n",
            "Model saved to models/es-EC_ibm1.pkl\n",
            "Processing es-HN...\n",
            "  Training on 115077 sentences...\n",
            "Model saved to models/es-HN_ibm1.pkl\n",
            "Processing es-NI...\n",
            "  Training on 115077 sentences...\n",
            "Model saved to models/es-NI_ibm1.pkl\n",
            "Processing es-PA...\n",
            "  Training on 115077 sentences...\n",
            "Model saved to models/es-PA_ibm1.pkl\n",
            "Processing es-PE...\n",
            "  Training on 115076 sentences...\n",
            "Model saved to models/es-PE_ibm1.pkl\n",
            "Processing es-PR...\n",
            "  Training on 115076 sentences...\n",
            "Model saved to models/es-PR_ibm1.pkl\n",
            "Processing es-SV...\n",
            "  Training on 115076 sentences...\n",
            "Model saved to models/es-SV_ibm1.pkl\n",
            "Processing es-UY...\n",
            "  Training on 115077 sentences...\n",
            "Model saved to models/es-UY_ibm1.pkl\n",
            "Processing es-VE...\n",
            "  Training on 115076 sentences...\n",
            "Model saved to models/es-VE_ibm1.pkl\n",
            "Processing std_es...\n"
          ]
        },
        {
          "ename": "TimeoutError",
          "evalue": "[Errno 60] Operation timed out",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(subdirs):\n\u001b[0;32m---> 43\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# if score is not None:\u001b[39;00m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;66;03m# results[d] = score\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(lang_dir, save_model_flag)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convenience function to train and evaluate a model for a language directory.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    BLEU score (float) or None if data not found\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (files not found or empty)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[2], line 12\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(lang_dir)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(en_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 12\u001b[0m     en_lines \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(es_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     14\u001b[0m     es_lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n",
            "\u001b[0;31mTimeoutError\u001b[0m: [Errno 60] Operation timed out"
          ]
        }
      ],
      "source": [
        "def train_and_evaluate(lang_dir, save_model_flag=False):\n",
        "    \"\"\"Convenience function to train and evaluate a model for a language directory.\n",
        "    \n",
        "    Args:\n",
        "        lang_dir: Language directory name\n",
        "        save_model_flag: Whether to save the trained model (default: False)\n",
        "    \n",
        "    Returns:\n",
        "        BLEU score (float) or None if data not found\n",
        "    \"\"\"\n",
        "    print(f\"Processing {lang_dir}...\")\n",
        "    data = load_data(lang_dir)\n",
        "    if not data:\n",
        "        print(f\"Skipping {lang_dir} (files not found or empty)\")\n",
        "        return None\n",
        "        \n",
        "    # Split data\n",
        "    random.seed(42)\n",
        "    random.shuffle(data)\n",
        "    split_idx = int(len(data) * 0.8)\n",
        "    train_data = data[:split_idx]\n",
        "    test_data = data[split_idx:]\n",
        "    \n",
        "    print(f\"  Training on {len(train_data)} sentences...\")\n",
        "    model = train(train_data, iterations=10)\n",
        "    \n",
        "    # Save model if requested\n",
        "    if save_model_flag:\n",
        "        model_path = os.path.join('models', f'{lang_dir}_ibm1.pkl')\n",
        "        save_model(model, model_path)\n",
        "    \n",
        "    # print(\"  Evaluating...\")\n",
        "    # score = evaluate(model, test_data)\n",
        "    # print(f\"  BLEU Score: {score:.4f}\")\n",
        "    return # score\n",
        "\n",
        "\n",
        "# Run for all datasets\n",
        "subdirs = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
        "results = {}\n",
        "\n",
        "for d in sorted(subdirs):\n",
        "    score = train_and_evaluate(d, True)\n",
        "    # if score is not None:\n",
        "        # results[d] = score\n",
        "        \n",
        "print(\"\\nFinal Results:\")\n",
        "# for lang, score in results.items():\n",
        "    # print(f\"{lang}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ds",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
