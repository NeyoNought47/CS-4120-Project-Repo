{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9c286ff3c07946a593f1d056c7833ab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19d93c7ba82f4544b1bebe93eb33c35d",
              "IPY_MODEL_a36e0cb637cb431b9b6b45bb345635ec",
              "IPY_MODEL_a2f80dbb00284e62bd8f24756e8fb4e3"
            ],
            "layout": "IPY_MODEL_f2d3d5e986cf48d3919e9f67a42ccbf8"
          }
        },
        "19d93c7ba82f4544b1bebe93eb33c35d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4781efba0a44602a8ecb3d531dc0b36",
            "placeholder": "​",
            "style": "IPY_MODEL_7dbd37e2b25d4d85a1ecab64ac1d7c7c",
            "value": "Map: 100%"
          }
        },
        "a36e0cb637cb431b9b6b45bb345635ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8967a014a76841128541fb3e6322a513",
            "max": 11574,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a3c1f8b4cce4839973858cf6c7c5e47",
            "value": 11574
          }
        },
        "a2f80dbb00284e62bd8f24756e8fb4e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d82d763830724028a2ed04a494c154a8",
            "placeholder": "​",
            "style": "IPY_MODEL_bb1fb048f2414269855486afd5f40e9c",
            "value": " 11574/11574 [00:01&lt;00:00, 7657.76 examples/s]"
          }
        },
        "f2d3d5e986cf48d3919e9f67a42ccbf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4781efba0a44602a8ecb3d531dc0b36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dbd37e2b25d4d85a1ecab64ac1d7c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8967a014a76841128541fb3e6322a513": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a3c1f8b4cce4839973858cf6c7c5e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d82d763830724028a2ed04a494c154a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb1fb048f2414269855486afd5f40e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0517c1bfa3d24dc99645afeb487809a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e49b7ee3f0894c1192e90393fc450a55",
              "IPY_MODEL_e076f96718e343bd99b813d29246fdaf",
              "IPY_MODEL_3b50207ab53e4c97a9b57c9d0657becc"
            ],
            "layout": "IPY_MODEL_ce686cf51d294c7fad345762924179b4"
          }
        },
        "e49b7ee3f0894c1192e90393fc450a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df232d51be9f47ef989a43bad763a494",
            "placeholder": "​",
            "style": "IPY_MODEL_a4198f2e130f4661ab95942b4a2c06bf",
            "value": "Map: 100%"
          }
        },
        "e076f96718e343bd99b813d29246fdaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_779258b648054710af3397c40cbc66a7",
            "max": 1286,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_147b56113a85454eb1f56c18e83af5a9",
            "value": 1286
          }
        },
        "3b50207ab53e4c97a9b57c9d0657becc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1af2637034746508a4cab14d211de08",
            "placeholder": "​",
            "style": "IPY_MODEL_65a042863c9d479782c6258ef634f6ac",
            "value": " 1286/1286 [00:00&lt;00:00, 6360.42 examples/s]"
          }
        },
        "ce686cf51d294c7fad345762924179b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df232d51be9f47ef989a43bad763a494": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4198f2e130f4661ab95942b4a2c06bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "779258b648054710af3397c40cbc66a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "147b56113a85454eb1f56c18e83af5a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1af2637034746508a4cab14d211de08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65a042863c9d479782c6258ef634f6ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c117fc7",
        "outputId": "4523f2ac-c671-4b38-ac7d-0b6eb59da64f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File structure\n",
        "'''mt5_HuggingFace/\n",
        "├── clean/\n",
        "│   ├── es-CL\n",
        "│   ├── es-CL\n",
        "│   └── ...\n",
        "└── mt5/\n",
        "    ├── mt5_finetune.ipynb\n",
        "    └── my_saved_mt5_model/'''"
      ],
      "metadata": {
        "id": "Vlsfcq-9DHqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "686450b8"
      },
      "source": [
        "# Instructions to Run This Notebook (Using Pre-trained Model)\n",
        "\n",
        "These instructions will guide you through running the notebook to use the *already saved* pre-trained MT5 model for translation, skipping the training steps to save 15+ minutes.\n",
        "\n",
        "### 1. Data and Notebook Access\n",
        "*   **Share the Saved Model:** Ensure the `mt5` folder containing the saved model and notebook, this is also the default folder to save the model.\n",
        "*   **Original Data (Optional):** The original data folder (`clean`) are only needed if you intend to run trainning model. __And you also need to modify the data path__.\n",
        "\n",
        "### 2. Everything Runs in Google Colab\n",
        "\n",
        "### 3. Mount Google Drive\n",
        "\n",
        "### 4. Verify Model Path\n",
        "*   Ensure that the `model_save_path` variable points to the intended location of saved model in Google Drive. Based on __my__ steps, this is `/content/drive/MyDrive/CS4120/mt5/my_saved_mt5_model`.\n",
        "\n",
        "### 5. Install Required Libraries\n",
        "\n",
        "### 6. Set Up GPU Runtime\n",
        "\n",
        "### 7. Run Necessary Cells in Order\n",
        "*   Since you're using a pre-trained model, you will skip the entire training process.\n",
        "*   **Minimum cells to run:**\n",
        "    *   **Mount Drive**\n",
        "    *   **Load Model & Tokenizer:** This cell should look something like this in my path:\n",
        "        ```python\n",
        "        model_path = \"/content/drive/MyDrive/CS4120/mt5/my_saved_mt5_model\"\n",
        "        tokenizer = T5TokenizerFast.from_pretrained(model_path)\n",
        "        model = MT5ForConditionalGeneration.from_pretrained(model_path)\n",
        "        print(f\"Model and tokenizer loaded from: {model_path}\")\n",
        "        ```\n",
        "    *   **Define `translate_mt5` function**\n",
        "    *   **Define decoding configs**\n",
        "    *   **Run translation examples**\n",
        "\n",
        "\n",
        "### 8. View Output\n",
        "*   The translation outputs will be printed directly below the relevant cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca56fd20"
      },
      "source": [
        "## Overall Notebook Logic and Process Flow\n",
        "\n",
        "Fine-tuning a pre-trained mT5 model for English to Spanish machine translation, specifically for dialectal variations found in Gnome project data\n",
        "\n",
        "1.  **Data Loading and Preparation**:\n",
        "    *   **Source Data**: It loads into Hugging Face `Dataset` objects.\n",
        "    *   **Dataset Addition**: Multiple dialectal datasets can be loaded and concatenated into a single `all_pairs`.\n",
        "    *   **Train/Validation Split**\n",
        "\n",
        "2.  **Model and Tokenizer Initialization**:\n",
        "    *   **Base Model**: A pre-trained `google/mt5-small` model and its corresponding `T5TokenizerFast` are loaded from Hugging Face Hub. mT5 (Massive Text-to-Text Transfer Transformer) is a multilingual encoder-decoder model suitable for translation tasks.\n",
        "    *   **Task Prefix**: A `task_prefix` (\"translate English to Spanish: \") is defined.\n",
        "    *   **Tokenization**: A `preprocess_batch` function is defined to tokenize both the English source and Spanish target sentences. __It also adds the task prefix to the English input.__\n",
        "\n",
        "3.  **Model Training**:\n",
        "    *   **Data Collator**: `DataCollatorForSeq2Seq`\n",
        "    *   **Training Arguments**: `Seq2SeqTrainingArguments`\n",
        "    *   **Trainer Setup**: A `Seq2SeqTrainer`\n",
        "    *   **Training Execution**: `trainer.train()`\n",
        "\n",
        "4.  **Model saved to a specified directory on Google Drive**\n",
        "\n",
        "5.  **Inference and Decoding Strategies**:\n",
        "    *   **`translate_mt5` Function**: performs translations. It takes an English text, the model, and tokenizer, along with various decoding parameters.\n",
        "        *   **Greedy Decoding**: Selects the most probable token at each step.\n",
        "        *   **Beam Search**: Keeps track of multiple probable sequences to find a globally better translation.\n",
        "        *   **Length Penalty**: Adjusts the likelihood of longer or shorter sequences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5150c4b"
      },
      "source": [
        "云端硬盘挂载成功后，请提供您要加载的数据文件的完整路径（例如，`/content/drive/My Drive/your_folder/your_file.csv`），我将帮助您将其加载到 pandas DataFrame 中。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Retrieval"
      ],
      "metadata": {
        "id": "Ni6qg_zIdWth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datasets import Dataset\n",
        "from transformers import MT5ForConditionalGeneration, T5TokenizerFast\n",
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import os\n",
        "import shutil # Import shutil for directory deletion\n"
      ],
      "metadata": {
        "id": "9Romv8Zle0iu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/CS4120/clean\"\n",
        "print(\"Folders:\", os.listdir(data_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is87c6KkbLVi",
        "outputId": "365c5cc5-60ae-465c-80f4-5a4f4c241336"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folders: ['es-CL', 'es-AR', 'std_es', 'es-VE', 'es-PA', 'es-PR', 'es-UY', 'es-CR', 'es-HN', 'es-CO', 'es-EC', 'es-DO', 'es-SV', 'es-PE', 'es-NI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "region_data = {}\n",
        "\n",
        "if os.path.exists(data_path):\n",
        "    sub_folders = sorted(os.listdir(data_path))\n",
        "\n",
        "    for folder_name in sub_folders:\n",
        "        folder_full_path = os.path.join(data_path, folder_name)\n",
        "\n",
        "        # Only process folders like es-AR, es-CO, ...\n",
        "        if os.path.isdir(folder_full_path) and folder_name.startswith(\"es-\"):\n",
        "\n",
        "            path_en = os.path.join(folder_full_path, \"all.en\")\n",
        "            path_es = os.path.join(folder_full_path, \"all.es\")\n",
        "\n",
        "            if os.path.exists(path_en) and os.path.exists(path_es):\n",
        "                with open(path_en, \"r\", encoding=\"utf-8\") as f:\n",
        "                    lines_en = f.read().strip().split(\"\\n\")\n",
        "\n",
        "                with open(path_es, \"r\", encoding=\"utf-8\") as f:\n",
        "                    lines_es = f.read().strip().split(\"\\n\")\n",
        "\n",
        "                current_pairs = []\n",
        "                if len(lines_en) == len(lines_es):\n",
        "                    for en, es in zip(lines_en, lines_es):\n",
        "                        if en.strip() and es.strip():\n",
        "                            current_pairs.append({\"en\": en.strip(), \"es\": es.strip()})\n",
        "\n",
        "                region_data[folder_name] = current_pairs\n",
        "\n",
        "print(\"Loaded regions:\", list(region_data.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZZVLJiSbeer",
        "outputId": "578f2f62-0373-482b-e3aa-ab6a87a1215e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded regions: ['es-AR', 'es-CL', 'es-CO', 'es-CR', 'es-DO', 'es-EC', 'es-HN', 'es-NI', 'es-PA', 'es-PE', 'es-PR', 'es-SV', 'es-UY', 'es-VE']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_pairs = []\n",
        "\n",
        "for region, pairs in region_data.items():\n",
        "    for p in pairs:\n",
        "        all_pairs.append({\n",
        "            \"input_text\": p[\"es\"],     # Spanish dialect sentence\n",
        "            \"target_text\": p[\"en\"],    # English sentence\n",
        "            \"region\": region           # Show the region\n",
        "        })\n",
        "\n",
        "print(\"Total training pairs:\", len(all_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtLAk2p1eYyp",
        "outputId": "3bbc9470-9eb8-4ec6-e1b7-89e9271eb071"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training pairs: 12860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset.from_list(all_pairs)\n",
        "\n",
        "dataset = dataset.train_test_split(test_size=0.1, shuffle=True)\n",
        "\n",
        "train_ds = dataset[\"train\"]\n",
        "val_ds = dataset[\"test\"]\n",
        "\n",
        "train_ds, test_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxaG5CKIeiGv",
        "outputId": "288ed2a0-71e4-4a08-c444-0c3dc74c6c1a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['input_text', 'target_text', 'region'],\n",
              "     num_rows: 11574\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['input_text', 'target_text', 'region'],\n",
              "     num_rows: 1286\n",
              " }))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# translation is like a text-to-text problem\n",
        "# input: en_sentence\n",
        "# output: es_sentence\n",
        "# import multilingual translation model and the tool needed to prepare text\n",
        "# the trainning process is to maximize the log-likelihood of the target sequence tokens (cross-entropy).\n",
        "model_name = \"google/mt5-small\"\n",
        "\n",
        "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "1gkOl8xgw2bX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# raw text -> token IDs for subword tokenization and SentencePiece\n",
        "# Rather than BoW or fixed-length vectors, the model sees a sequence of (subword) indices;\n",
        "# the transformer turns them into contextual embeddings via self-attention.\n",
        "\n",
        "max_source_length = 128\n",
        "max_target_length = 128\n",
        "task_prefix = \"translate English to Spanish: \"\n",
        "\n",
        "def preprocess_batch(batch):\n",
        "    # 1. Build the input (source) text\n",
        "    inputs = [task_prefix + s for s in batch[\"input_text\"]]\n",
        "    targets = batch[\"target_text\"]\n",
        "\n",
        "    # 2. Tokenize inputs\n",
        "    # to convert both English inputs and Spanish targets into numerical token IDs\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=max_source_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    # 3. Tokenize targets (labels)\n",
        "    # It sets the tokenized Spanish sentences as labels for the model to learn from\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            targets,\n",
        "            max_length=max_target_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# use batch and remove the original text col\n",
        "train_tokenized = train_ds.map(\n",
        "    preprocess_batch,\n",
        "    batched=True,\n",
        "    remove_columns=train_ds.column_names,\n",
        ")\n",
        "\n",
        "val_tokenized = val_ds.map(\n",
        "    preprocess_batch,\n",
        "    batched=True,\n",
        "    remove_columns=val_ds.column_names,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135,
          "referenced_widgets": [
            "9c286ff3c07946a593f1d056c7833ab3",
            "19d93c7ba82f4544b1bebe93eb33c35d",
            "a36e0cb637cb431b9b6b45bb345635ec",
            "a2f80dbb00284e62bd8f24756e8fb4e3",
            "f2d3d5e986cf48d3919e9f67a42ccbf8",
            "a4781efba0a44602a8ecb3d531dc0b36",
            "7dbd37e2b25d4d85a1ecab64ac1d7c7c",
            "8967a014a76841128541fb3e6322a513",
            "3a3c1f8b4cce4839973858cf6c7c5e47",
            "d82d763830724028a2ed04a494c154a8",
            "bb1fb048f2414269855486afd5f40e9c",
            "0517c1bfa3d24dc99645afeb487809a2",
            "e49b7ee3f0894c1192e90393fc450a55",
            "e076f96718e343bd99b813d29246fdaf",
            "3b50207ab53e4c97a9b57c9d0657becc",
            "ce686cf51d294c7fad345762924179b4",
            "df232d51be9f47ef989a43bad763a494",
            "a4198f2e130f4661ab95942b4a2c06bf",
            "779258b648054710af3397c40cbc66a7",
            "147b56113a85454eb1f56c18e83af5a9",
            "d1af2637034746508a4cab14d211de08",
            "65a042863c9d479782c6258ef634f6ac"
          ]
        },
        "id": "Kz--ewrzy_So",
        "outputId": "e76a7619-a204-4001-b21a-9544ed7dc08b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/11574 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c286ff3c07946a593f1d056c7833ab3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4118: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1286 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0517c1bfa3d24dc99645afeb487809a2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use Seq2SeqTrainer for encoder-decoder models\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "# Seq2SeqTrainingArguments defines all the hyperparameters and strategies for training.\n",
        "# These arguments control various aspects of the training loop.\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"mt5-gnome-en-es\",   # Directory where model checkpoints and logs will be saved.\n",
        "    per_device_train_batch_size=4,  # Batch size for training on each device (GPU/CPU).\n",
        "    per_device_eval_batch_size=4,   # Batch size for evaluation on each device.\n",
        "    learning_rate=3e-4,             # The initial learning rate for the optimizer.\n",
        "    num_train_epochs=3,             # Total number of training epochs to perform.\n",
        "    logging_steps=100,              # Number of update steps between two logs.\n",
        "    eval_strategy=\"epoch\",          # Evaluate the model at the end of each epoch.\n",
        "    save_strategy=\"epoch\",          # Save the model checkpoint at the end of each epoch.\n",
        "    predict_with_generate=True,     # Whether to use generate to calculate metrics (useful for sequence generation tasks).\n",
        "    fp16=False,                     # Whether to use mixed precision training (float16). Set to True for performance on compatible GPUs.\n",
        ")"
      ],
      "metadata": {
        "id": "AoCHR85Dzf7N"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply LR and neural LMs, and minimize cross-entropy\n",
        "# coder–decoder transformer that learns a conditional distribution P(Spanish token | English tokens)\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWefp0qh1id6",
        "outputId": "85cfff0c-da00-42a6-cd12-80158a64505c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1775927734.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# API key will be required here\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "l0R2JF0b187x",
        "outputId": "a30d0304-b60e-4e41-f341-12a0433d4f77"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mczzzttttt1\u001b[0m (\u001b[33mczzzttttt1-northeastern-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251203_045551-qh3ugw37</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/czzzttttt1-northeastern-university/huggingface/runs/qh3ugw37' target=\"_blank\">denim-wood-4</a></strong> to <a href='https://wandb.ai/czzzttttt1-northeastern-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/czzzttttt1-northeastern-university/huggingface' target=\"_blank\">https://wandb.ai/czzzttttt1-northeastern-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/czzzttttt1-northeastern-university/huggingface/runs/qh3ugw37' target=\"_blank\">https://wandb.ai/czzzttttt1-northeastern-university/huggingface/runs/qh3ugw37</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8682' max='8682' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8682/8682 28:24, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.480300</td>\n",
              "      <td>0.186883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.174300</td>\n",
              "      <td>0.043479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.082300</td>\n",
              "      <td>0.026280</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=8682, training_loss=0.9239993244237928, metrics={'train_runtime': 1835.5689, 'train_samples_per_second': 18.916, 'train_steps_per_second': 4.73, 'total_flos': 846625330237440.0, 'train_loss': 0.9239993244237928, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the Google Drive path where to save the model\n",
        "model_save_path = \"/content/drive/MyDrive/CS4120/mt5/my_saved_mt5_model\"\n",
        "\n",
        "# Ensure the parent directory exists\n",
        "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
        "\n",
        "# Check if the directory already exists and delete it to ensure a clean save\n",
        "if os.path.exists(model_save_path):\n",
        "    print(f\"Deleting existing directory: '{model_save_path}' to ensure a clean save.\")\n",
        "    shutil.rmtree(model_save_path)\n",
        "\n",
        "# Recreate the directory after deletion\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "# Save the trained model and tokenizer to the specified path\n",
        "trainer.save_model(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"Model and tokenizer saved to: {model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gewVPJSL2Zkq",
        "outputId": "a2f6fdbf-5cb3-416f-a8fe-13fe949dd7f9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleting existing directory: '/content/drive/MyDrive/CS4120/mt5/my_saved_mt5_model' to ensure a clean save.\n",
            "Model and tokenizer saved to: /content/drive/MyDrive/CS4120/mt5/my_saved_mt5_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Start to run from the below if you DON'T want to retrain the model(otherwise, it may take at least 15 minutes).**"
      ],
      "metadata": {
        "id": "APNOfHzq1LCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If there's error to load the saved model, you may need to downgrade the colab.\n",
        "# This is for compatibility issue\n",
        "'''import transformers\n",
        "print(transformers.__version__)\n",
        "\n",
        "!pip install -q \"transformers==4.57.1\"'''"
      ],
      "metadata": {
        "id": "ZpT3EMcA3vy5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9e09584a-3729-447b-99ab-ede4e914b5fe"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import transformers\\nprint(transformers.__version__)\\n\\n!pip install -q \"transformers==4.57.1\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec825cd2",
        "outputId": "654270ee-b4d2-4324-ad6d-e4150b9a47a1"
      },
      "source": [
        "from transformers import MT5ForConditionalGeneration, T5TokenizerFast\n",
        "import os\n",
        "\n",
        "# Load the tokenizer and model from the saved directory\n",
        "model_path = \"/content/drive/MyDrive/CS4120/mt5/my_saved_mt5_model\"\n",
        "\n",
        "# Diagnostic code to check model_path contents with error checking\n",
        "print(f\"Attempting to load model from: {model_path}\")\n",
        "if not os.path.exists(model_path):\n",
        "    print(f\"Error: Model path '{model_path}' does not exist. Please ensure the model was saved correctly and Google Drive is mounted.\")\n",
        "elif not os.listdir(model_path):\n",
        "    print(f\"Error: Model path '{model_path}' is empty. Please ensure the model was saved completely.\")\n",
        "else:\n",
        "    print(f\"Contents of '{model_path}':\")\n",
        "    for item in os.listdir(model_path):\n",
        "        print(f\"  - {item}\")\n",
        "\n",
        "# Load model and tokenizer with error checking\n",
        "# You're expected to see \"Model and tokenizer loaded successfully from:...\"\n",
        "try:\n",
        "    tokenizer = T5TokenizerFast.from_pretrained(model_path)\n",
        "    model = MT5ForConditionalGeneration.from_pretrained(model_path)\n",
        "    print(f\"Model and tokenizer loaded successfully from: {model_path}\")\n",
        "except AttributeError as e:\n",
        "    print(f\"\\nAn AttributeError occurred during model/tokenizer loading: {e}\")\n",
        "    print(\"This often happens if the configuration files (e.g., config.json, tokenizer_config.json) are missing or corrupted in the saved directory.\")\n",
        "    print(\"Please ensure the model was saved completely and correctly to the specified path, and try re-running the save cell first.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn unexpected error occurred during model/tokenizer loading: {e}\")\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load model from: /content/drive/MyDrive/CS4120/mt5/my_saved_mt5_model\n",
            "Contents of '/content/drive/MyDrive/CS4120/mt5/my_saved_mt5_model':\n",
            "  - config.json\n",
            "  - generation_config.json\n",
            "  - model.safetensors\n",
            "  - tokenizer_config.json\n",
            "  - special_tokens_map.json\n",
            "  - spiece.model\n",
            "  - tokenizer.json\n",
            "  - training_args.bin\n",
            "\n",
            "An AttributeError occurred during model/tokenizer loading: 'dict' object has no attribute 'model_type'\n",
            "This often happens if the configuration files (e.g., config.json, tokenizer_config.json) are missing or corrupted in the saved directory.\n",
            "Please ensure the model was saved completely and correctly to the specified path, and try re-running the save cell first.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decoding process to find y_hat = argmaxP(y|x)\n",
        "#\tSo use heuristics:\n",
        "#\tGreedy: at each step take the most probable next token.\n",
        "#\tBeam search: keep the top k partial sequences (beam size), expand each, keep top k again.\n",
        "#\tAdd length penalties to avoid over-favoring short sequences.\n",
        "\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def translate_mt5(\n",
        "    text_en, # The English text to be translated\n",
        "    model,   # The MT5 model used for translation\n",
        "    tokenizer, # The tokenizer corresponding to the MT5 model\n",
        "    num_beams=1, # Number of beams for beam search. 1 means greedy decoding.\n",
        "    do_sample=False, # Whether to use sampling; False for deterministic decoding (beam search/greedy)\n",
        "    max_length=128, # Maximum length of the generated target sequence\n",
        "    length_penalty=1, # Penalty for generating longer sequences\n",
        "    temperature=1, # Controls randomness in sampling. Lower values make output more deterministic.\n",
        "    top_p=None, # Top-p (nucleus) sampling parameter\n",
        "):\n",
        "    # Prepare the input text with the task prefix\n",
        "    input_text = task_prefix + text_en\n",
        "    # Tokenize the input text and move it to the appropriate device (CPU/GPU)\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\", # Return PyTorch tensors\n",
        "        truncation=True,     # Truncate sequences longer than max_source_length\n",
        "        max_length=max_source_length,\n",
        "    ).to(device)\n",
        "\n",
        "    # Define generation arguments\n",
        "    gen_kwargs = {\n",
        "        \"max_length\": max_length,\n",
        "        \"num_beams\": num_beams,\n",
        "        \"length_penalty\": length_penalty,\n",
        "        \"do_sample\": do_sample,\n",
        "        \"temperature\": temperature,\n",
        "    }\n",
        "\n",
        "    # Add top_p to generation arguments if specified\n",
        "    if top_p is not None:\n",
        "        gen_kwargs[\"top_p\"] = top_p\n",
        "\n",
        "    # Generate the output sequence (translated text token IDs)\n",
        "    output_ids = model.generate(**inputs, **gen_kwargs)\n",
        "    # Decode the generated token IDs back into human-readable text, skipping special tokens\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "0ce3mNOI9y-L"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoding_configs = [\n",
        "    {\"name\": \"greedy\",        \"num_beams\": 1, \"do_sample\": False, \"length_penalty\": 1.0},\n",
        "    {\"name\": \"beam_4\",        \"num_beams\": 4, \"do_sample\": False, \"length_penalty\": 1.0},\n",
        "    {\"name\": \"beam_8\",        \"num_beams\": 8, \"do_sample\": False, \"length_penalty\": 1.0},\n",
        "    {\"name\": \"beam_4_lp_0.6\", \"num_beams\": 4, \"do_sample\": False, \"length_penalty\": 0.6},\n",
        "    {\"name\": \"beam_4_lp_1.4\", \"num_beams\": 4, \"do_sample\": False, \"length_penalty\": 1.4},\n",
        "    # Optional:\n",
        "    # {\"name\": \"top_p_0.9\", \"num_beams\": 1, \"do_sample\": True,  \"top_p\": 0.9, \"temperature\": 0.7},\n",
        "]"
      ],
      "metadata": {
        "id": "8WT_HD4aEMRH"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defind those variable again in here(if you want to use the existing saved model rather than retrainning the model)\n",
        "max_source_length = 128\n",
        "max_target_length = 128\n",
        "task_prefix = \"translate English to Spanish: \""
      ],
      "metadata": {
        "id": "Wogbi5hs0sjT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some test sentences to be translated\n",
        "test_examples = [\n",
        "    \"Keyboard Accessibility Preferences\",\n",
        "    \"Shows the status of keyboard accessibility features\",\n",
        "    \"There was an error launching the help viewer.\",\n",
        "]\n",
        "\n",
        "# Apply the model and get the translation with customized parameters(beam_# ...)\n",
        "for text in test_examples:\n",
        "    print(f\"\\nSOURCE: {text}\")\n",
        "    for cfg in decoding_configs:\n",
        "        out = translate_mt5(\n",
        "            text_en=text,\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            num_beams=cfg.get(\"num_beams\", 1),\n",
        "            do_sample=cfg.get(\"do_sample\", False),\n",
        "            length_penalty=cfg.get(\"length_penalty\", 1.0),\n",
        "            temperature=cfg.get(\"temperature\", 1.0),\n",
        "            top_p=cfg.get(\"top_p\", None),\n",
        "        )\n",
        "        print(f\"[{cfg['name']}] {out}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tU_1i-2yFRQc",
        "outputId": "58138a0a-c458-40ce-c281-d5294afcc47d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SOURCE: Keyboard Accessibility Preferences\n",
            "[greedy] Default Keyboard\n",
            "[beam_4] Default Keyboard\n",
            "[beam_8] Default Keyboard\n",
            "[beam_4_lp_0.6] Default Keyboard\n",
            "[beam_4_lp_1.4] Default Keyboard\n",
            "\n",
            "SOURCE: Shows the status of keyboard accessibility features\n",
            "[greedy] Shows the status of keyboard accessibility features\n",
            "[beam_4] Shows the status of keyboard accessibility features\n",
            "[beam_8] Shows the status of keyboard accessibility features\n",
            "[beam_4_lp_0.6] Shows the status of keyboard accessibility features\n",
            "[beam_4_lp_1.4] Shows the status of keyboard accessibility features\n",
            "\n",
            "SOURCE: There was an error launching the help viewer.\n",
            "[greedy] There was an error launching the help viewer.\n",
            "[beam_4] There was an error launching the help viewer.\n",
            "[beam_8] There was an error launching the help viewer.\n",
            "[beam_4_lp_0.6] There was an error launching the help viewer.\n",
            "[beam_4_lp_1.4] There was an error launching the help viewer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output explanation:\n",
        "\n",
        "1. Greedy: the model simply picks the word with the highest probability as the next word in the sequence. It can be suboptimal because a locally optimal choice at one step might lead to a globally bad translation later on.\n",
        "2. Beam: Instead of just picking the single best word at each step, beam search keeps track of the num_beams (e.g., 4 or 8) most probable partial translations. Therefore, __it's less likely to get stuck in local optima. Increasing num_beams usually leads to better quality, up to a point.__\n",
        "3. lp_#: This parameter is used with beam search to influence the length of the generated translation. Models sometimes have a bias towards generating shorter sequences. A higher number will encourage to generate longer sequences. However, __if the outputs are the same, it means the length penalties doesn't alter the most probable sequence for this model__."
      ],
      "metadata": {
        "id": "ixAGptd2WKkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End of code"
      ],
      "metadata": {
        "id": "b_L4EMn8MhuY"
      }
    }
  ]
}